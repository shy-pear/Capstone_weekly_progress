{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6adf6814-0a19-48cc-a46a-6f96d0e264fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81a4dbd-e16f-48ab-a0f4-8b050dc4967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d78592e-c244-4f31-90cd-d65cc8cfca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9cbf191-f292-42be-94da-e049c70f1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e68849b-af0a-40bb-848b-71138e76d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83896e5d-3611-4e0a-b31d-d41431dda4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446a7983-cabd-441e-a6a0-20a2a28d677c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"prajjwal1/bert-mini\"  # Optimized small model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL)\n",
    "\n",
    "# Load Model\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL, num_labels=2)  # Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00c02af2-4c83-4568-8932-3dbeefd98c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dataset = load_dataset(\"imdb\")  # Sentiment analysis dataset\n",
    "suicide_dataset = load_dataset(\"vibhorag101/suicide_prediction_dataset_phr\")  # Suicide detection dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f051aef3-7b90-4248-bcbd-7a024ceb4e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 185574\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 46394\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(suicide_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58f17262-f86d-43ee-ad16-013747121b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f389a6-1afa-4926-85e8-af333b6be380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert string labels to numeric values\n",
    "def map_labels(example):\n",
    "    # SuicideWatch dataset: Convert \"suicide\" â†’ 1, \"non-suicide\" â†’ 0\n",
    "    if example[\"label\"] == \"suicide\":\n",
    "        example[\"label\"] = 1\n",
    "    elif example[\"label\"] == \"non-suicide\":\n",
    "        example[\"label\"] = 0\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d120bb4-1c7a-4db7-aaaa-14c06b71246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "suicide_dataset = suicide_dataset.map(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "715141a5-986b-481c-991a-878b79f5e44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5960b77aedc24cbc9caa6014500f643d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/185574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43279fec9c54ad4a5e0924f0495f778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize and Rename Labels Efficiently\n",
    "def preprocess_function(batch):\n",
    "    tokenized = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    tokenized[\"labels\"] = [int(label) for label in batch[\"label\"]]\n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization to both datasets\n",
    "suicide_dataset = suicide_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04945a6c-6b88-41f5-a14b-ff21df3728ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7766d3f4ebf34f809588a2e38654386e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11c6fcd0b4741ac98e8fc6d6991dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd282e9cd4646b1a265a54c4c7f588c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiment_dataset = sentiment_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31eccb9a-0d36-45a8-a64b-2546da2d9396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove text column (no longer needed)\n",
    "suicide_dataset = suicide_dataset.remove_columns([\"text\"])\n",
    "sentiment_dataset = sentiment_dataset.remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb8da07f-11df-49d2-8c20-11434892bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'input_ids': [101, 2342, 2203, 6114, 4895, 4783, 5400, 6321, 5051, 27469, 2425, 2994, 2113, 2131, 2488, 3984, 2025, 2025, 2113, 2514, 2051, 16873, 5920, 14337, 16592, 2135, 2025, 16592, 2135, 2025, 2191, 2514, 2488, 2215, 2203, 9826, 2699, 2673, 2052, 2191, 2488, 2920, 2542, 2498, 2499, 2215, 3280, 2342, 3280, 2025, 10107, 9015, 2172, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    }
   ],
   "source": [
    "print(suicide_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afb61be2-8c05-4b34-ab68-30a1489537c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Dataset Wrapper\n",
    "class MultiTaskDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(item[\"labels\"], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Wrap datasets\n",
    "train_suicide_dataset = MultiTaskDataset(suicide_dataset[\"train\"])\n",
    "test_suicide_dataset = MultiTaskDataset(suicide_dataset[\"test\"])\n",
    "\n",
    "train_sentiment_dataset = MultiTaskDataset(sentiment_dataset[\"train\"])\n",
    "test_sentiment_dataset = MultiTaskDataset(sentiment_dataset[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df76d4ca-15dd-4255-acff-2b10094f3279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MultiTaskDataset object at 0x137503e30>\n"
     ]
    }
   ],
   "source": [
    "print(train_suicide_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbb03c6b-31a7-41f0-af0f-de240f6c5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create efficient dataloaders\n",
    "BATCH_SIZE = 8  # Increase batch size for efficiency\n",
    "\n",
    "train_suicide_loader = DataLoader(train_suicide_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_suicide_loader = DataLoader(test_suicide_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_sentiment_loader = DataLoader(train_sentiment_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_sentiment_loader = DataLoader(test_sentiment_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05f2ecd1-a275-4553-a64c-660d0d305c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Device (Supports Mac MPS and CUDA)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4007463-6a50-4ffd-9942-8588e9e9d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "745c05cd-d9bc-41d7-b076-d3edc62631cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer & Loss Function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)  # Higher learning rate for BERT-Mini\n",
    "loss_fn = nn.CrossEntropyLoss()  # Binary classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9871d3a-e4d3-47a6-a1b8-166fd459a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model optimizations for less memory usage and better training\n",
    "\n",
    "# Less dropout layers\n",
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.Dropout):\n",
    "        module.p = 0.05  # Reduce dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64967647-e48e-4d67-ae25-945b095cb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.LayerNorm(256),  # Normalize before classification\n",
    "    nn.Linear(256, 2)   # Keep original classifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ede0af4c-1714-486d-8798-a6ad26303510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.05, inplace=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.half()  # Convert model weights to float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b94c87ed-7219-4051-8715-aa3bff477333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configurations\n",
    "EPOCHS = 3  # More epochs compensate for smaller model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d2992c4-14cc-463a-a7ff-f118bf344caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2131, 24209,  ...,     0,     0,     0],\n",
      "        [  101,  5962,  7477,  ...,     0,     0,     0],\n",
      "        [  101,  2699,  4830,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2601,  2617,  ...,     0,     0,     0],\n",
      "        [  101,  3087,  2842,  ...,     0,     0,     0],\n",
      "        [  101,  2267, 24665,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 0, 0, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_suicide_loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "038e8744-8c2b-4f0a-ad3d-60742a33d4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e11ff91e-90f4-4d78-9195-6e17bd8beb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is fully on the correct device\n",
    "model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.to(device)\n",
    "    if param.grad is not None:\n",
    "        param.grad.data = param.grad.data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "492fdbc1-5f6b-473f-aa63-780a638abcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training...\n",
      "Batch 0/3125 - Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m total_loss = (loss_suicide + loss_sentiment) / \u001b[32m2\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m     29\u001b[39m optimizer.step()\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Logging Progress\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DetectionBot/virtual_env/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DetectionBot/virtual_env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DetectionBot/virtual_env/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    num_batches = min(len(train_suicide_loader), len(train_sentiment_loader))  # Ensure equal batches\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} - Training...\")\n",
    "\n",
    "    for batch_idx, (batch_suicide, batch_sentiment) in enumerate(zip(train_suicide_loader, train_sentiment_loader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Suicide Task\n",
    "        inputs = {key: val.to(device) for key, val in batch_suicide.items() if key in [\"input_ids\", \"attention_mask\"]}\n",
    "        labels = batch_suicide[\"labels\"].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        loss_suicide = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        # Sentiment Task\n",
    "        inputs = {key: val.to(device) for key, val in batch_sentiment.items() if key in [\"input_ids\", \"attention_mask\"]}\n",
    "        labels = batch_sentiment[\"labels\"].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        loss_sentiment = loss_fn(outputs.logits, labels)\n",
    "\n",
    "        # Combine Losses\n",
    "        total_loss = (loss_suicide + loss_sentiment) / 2\n",
    "\n",
    "        # Backpropagation\n",
    "        total_loss.backward()  \n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging Progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Batch {batch_idx}/{num_batches} - Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed. Avg Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e378fab8-2589-4c3e-aa66-f529aca56f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "MODEL_PATH = \"./trained_suicide_detection_model_bertmini\"  # saved in same folder\n",
    "# Ensure directory exists\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ed7365a-a16a-4786-bb1b-6baf62d6a410",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_suicide_detection_model_bertmini/tokenizer_config.json',\n",
       " './trained_suicide_detection_model_bertmini/special_tokens_map.json',\n",
       " './trained_suicide_detection_model_bertmini/vocab.txt',\n",
       " './trained_suicide_detection_model_bertmini/added_tokens.json',\n",
       " './trained_suicide_detection_model_bertmini/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(MODEL_PATH)\n",
    "tokenizer.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04b9368f-8aab-4354-b316-e6b23315fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3c5cfc7-24c0-461d-8a19-2b8982b83b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1b05021-b371-4f51-8651-a9fce32675c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded from: ./trained_suicide_detection_model_bertmini\n"
     ]
    }
   ],
   "source": [
    "print(\"Model successfully loaded from:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f26af038-387e-4d29-bf05-67a871291627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Non-Suicidal\n",
      "Confidence: [[0.5663725  0.43362752]]\n"
     ]
    }
   ],
   "source": [
    "# Test on new messages\n",
    "def predict_suicide_risk(text):\n",
    "    \"\"\"Runs a prediction on a single text input\"\"\"\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Move to the same device as the model\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Get prediction label\n",
    "    predicted_label = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "    # Map label to class\n",
    "    label_map = {0: \"Non-Suicidal\", 1: \"High Suicide Risk\"}\n",
    "    return label_map[predicted_label], probs.cpu().numpy()\n",
    "\n",
    "# Example Test\n",
    "text_input = \"I want to hurt myself.\"\n",
    "prediction, confidence = predict_suicide_risk(text_input)\n",
    "\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35e2ddcd-1337-44ac-a884-9a75db04fc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suicide Classification: [[{'label': 'LABEL_0', 'score': 0.5520013570785522}]]\n",
      "Sentiment Classification: [[{'label': 'LABEL_0', 'score': 0.5520013570785522}]]\n"
     ]
    }
   ],
   "source": [
    "# Load Model for Testing\n",
    "from transformers import pipeline\n",
    "\n",
    "suicide_classifier = pipeline(\"text-classification\", model=MODEL_PATH, top_k=1)\n",
    "sentiment_classifier = pipeline(\"text-classification\", model=MODEL_PATH, top_k=1)\n",
    "\n",
    "# Test\n",
    "test_message = \"I feel hopeless and don't want to live.\"\n",
    "suicide_result = suicide_classifier(test_message)\n",
    "sentiment_result = sentiment_classifier(test_message)\n",
    "\n",
    "print(f\"Suicide Classification: {suicide_result}\")\n",
    "print(f\"Sentiment Classification: {sentiment_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51009446-b21b-442f-8826-ae10c4d91b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on suicide test data\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "# Total number of batches in test dataset\n",
    "num_batches = len(test_suicide_loader)\n",
    "\n",
    "print(f\"Evaluating on {num_batches} batches...\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_suicide_loader):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key in [\"input_ids\", \"attention_mask\"]}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get predicted class (0 = Non-Suicidal, 1 = Suicidal)\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "        # ðŸ”¹ Print progress every 100 batches\n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == num_batches:\n",
    "            print(f\"Processed {batch_idx + 1}/{num_batches} batches...\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Total Time for Testing: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Non-Suicidal\", \"Suicidal\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f3df910-1fd7-496c-a288-192e73ed4723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Non-Suicidal\n",
      "Confidence: [[0.63923585 0.3607641 ]]\n"
     ]
    }
   ],
   "source": [
    "# Test on new messages\n",
    "def predict_suicide_risk(text):\n",
    "    \"\"\"Runs a prediction on a single text input\"\"\"\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Move to the same device as the model\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Get prediction label\n",
    "    predicted_label = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "    # Map label to class\n",
    "    label_map = {0: \"Non-Suicidal\", 1: \"High Suicide Risk\"}\n",
    "    return label_map[predicted_label], probs.cpu().numpy()\n",
    "\n",
    "# Example Test\n",
    "text_input = \"I want to hurt myself.\"\n",
    "prediction, confidence = predict_suicide_risk(text_input)\n",
    "\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Confidence: {confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b043f2b-a5b8-4dc6-bfa2-25a7fdc64a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
